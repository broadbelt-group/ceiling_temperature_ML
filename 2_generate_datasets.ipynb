{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea8b7e5",
   "metadata": {},
   "source": [
    "## 2. Mark Global Test Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit.ML.Cluster import Butina\n",
    "import os\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed107b",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c2fe12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"0_raw_data_sets/featurized_imputed_data.csv\"\n",
    "df = pd.read_csv(filename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6ac89ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['BASE_Category'] != 'misc']\n",
    "df[\"dH (kJ/mol)\"] = df[\"dH (kJ/mol)\"] / 4.184\n",
    "df[\"dS (J/mol/K)\"] = df[\"dS (J/mol/K)\"] / 4.184\n",
    "\n",
    "df = df.rename(columns={'dH (kJ/mol)': 'dH (kcal/mol)', 'dS (J/mol/K)': 'dS (cal/mol/K)'})\n",
    "df = df.round(3)\n",
    "\n",
    "# Making sure that everything is unique\n",
    "tags = []\n",
    "for _, row in df.iterrows():\n",
    "    tag = row['BASE_State'] + row['BASE_Category'] + row['Canonical SMILES'] + str(row['Solvent'])\n",
    "    tags.append(tag)\n",
    "assert(len(tags) == len(set(tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b1c7e489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "Solvent\n",
      "dH (kcal/mol)\n",
      "dS (cal/mol/K)\n",
      "Solvent_SMILES\n",
      "Solvent_SMILES_2\n",
      "SOLV_PARAM_s_g\n",
      "SOLV_PARAM_b_g\n",
      "SOLV_PARAM_e_g\n",
      "SOLV_PARAM_l_g\n",
      "SOLV_PARAM_a_g\n",
      "SOLV_PARAM_c_g\n",
      "SOLV_PARAM_visc at 298 K (cP)\n",
      "SOLV_PARAM_dielectric constant\n"
     ]
    }
   ],
   "source": [
    "# print names of columns in combined_df that have any missing values\n",
    "missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "print(\"Columns with missing values:\")\n",
    "for col in missing_cols:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a511bc",
   "metadata": {},
   "source": [
    "### Determine Global Test Set Based on Butina Clustering/Phase Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_global_test(df, test_fraction=0.10, phase_col='BASE_Monomer_State', smiles_col='Canonical SMILES', h_col='dH (kcal/mol)', s_col='dS (cal/mol/K)', \n",
    "                         radius=2, nBits=2048, seed=42, n_h_bins=5, n_s_bins=5):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    df = df.copy()\n",
    "    df['Global_Test'] = False\n",
    "    \n",
    "    # Filter molecules with both H and S (negative)\n",
    "    df['Tc C'] = df['dH (kcal/mol)']*1000/df['dS (cal/mol/K)']-273\n",
    "    eligible_idx = df.index[(df[h_col].notna()) & (df[s_col].notna()) & (df['Tc C'] < 1000) & (df['Tc C'] > -500)]\n",
    "    eligible_df = df.loc[eligible_idx]\n",
    "    \n",
    "    # Create bins for H and S\n",
    "    eligible_df = eligible_df.copy()\n",
    "    eligible_df['H_bin'] = pd.qcut(eligible_df[h_col], q=n_h_bins, labels=False, duplicates='drop')\n",
    "    eligible_df['S_bin'] = pd.qcut(eligible_df[s_col], q=n_s_bins, labels=False, duplicates='drop')\n",
    "    eligible_df['HS_bin'] = eligible_df['H_bin'].astype(str) + '_' + eligible_df['S_bin'].astype(str)\n",
    "    \n",
    "    # Generate fingerprints\n",
    "    mols = [Chem.MolFromSmiles(s) for s in eligible_df[smiles_col]]\n",
    "    gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=nBits)\n",
    "    fps = [gen.GetFingerprint(m) for m in mols]\n",
    "    n = len(fps)\n",
    "    \n",
    "    dists = []\n",
    "    for i in range(1, n):\n",
    "        sims = DataStructs.BulkTanimotoSimilarity(fps[i], fps[:i])\n",
    "        dists.extend([1 - x for x in sims])\n",
    "    \n",
    "    # Cluster using Butina\n",
    "    clusters = Butina.ClusterData(dists, n, 0.2, isDistData=True)\n",
    "    cluster_map = {i: [eligible_df.index[j] for j in cluster] for i, cluster in enumerate(clusters)}\n",
    "    \n",
    "    # Compute counts\n",
    "    phase_values = eligible_df[phase_col].unique()\n",
    "    h_bin_values = sorted(eligible_df['H_bin'].dropna().unique())\n",
    "    s_bin_values = sorted(eligible_df['S_bin'].dropna().unique())\n",
    "    \n",
    "    h_bin_counts = eligible_df['H_bin'].value_counts().to_dict()\n",
    "    s_bin_counts = eligible_df['S_bin'].value_counts().to_dict()\n",
    "    \n",
    "    target_test_count = int(len(eligible_df) * test_fraction)\n",
    "    \n",
    "    # Iteratively select clusters\n",
    "    cluster_ids = list(cluster_map.keys())\n",
    "    rng.shuffle(cluster_ids)\n",
    "    \n",
    "    test_counts_h = {b: 0 for b in h_bin_values}\n",
    "    test_counts_s = {b: 0 for b in s_bin_values}\n",
    "    total_test = 0\n",
    "    selected_idxs = []\n",
    "    \n",
    "    for cid in cluster_ids:\n",
    "        idxs = cluster_map[cid]\n",
    "        cluster_h_bins = eligible_df.loc[idxs, 'H_bin']\n",
    "        cluster_s_bins = eligible_df.loc[idxs, 'S_bin']\n",
    "        add_cluster = True\n",
    "        \n",
    "        for h_bin in cluster_h_bins.dropna().unique():\n",
    "            n_h_in_cluster = (cluster_h_bins == h_bin).sum()\n",
    "            n_h_total = h_bin_counts.get(h_bin, 1)\n",
    "            if test_counts_h[h_bin] + n_h_in_cluster > 2.0 * test_fraction * n_h_total:\n",
    "                add_cluster = False\n",
    "                break\n",
    "        \n",
    "        if add_cluster:\n",
    "            for s_bin in cluster_s_bins.dropna().unique():\n",
    "                n_s_in_cluster = (cluster_s_bins == s_bin).sum()\n",
    "                n_s_total = s_bin_counts.get(s_bin, 1)\n",
    "                if test_counts_s[s_bin] + n_s_in_cluster > 2.0 * test_fraction * n_s_total:\n",
    "                    add_cluster = False\n",
    "                    break\n",
    "        \n",
    "        if add_cluster and total_test + len(idxs) <= target_test_count:\n",
    "            df.loc[idxs, 'Global_Test'] = True\n",
    "            total_test += len(idxs)\n",
    "            selected_idxs.extend(idxs)\n",
    "            \n",
    "            for h_bin in cluster_h_bins.dropna().unique():\n",
    "                test_counts_h[h_bin] += (cluster_h_bins == h_bin).sum()\n",
    "            for s_bin in cluster_s_bins.dropna().unique():\n",
    "                test_counts_s[s_bin] += (cluster_s_bins == s_bin).sum()\n",
    "        \n",
    "        if total_test >= target_test_count:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTotal eligible molecules: {len(eligible_df)}\")\n",
    "    print(f\"Global test set size: {df['Global_Test'].sum()}\")\n",
    "    print(f\"Global test fraction: {df['Global_Test'].sum() / len(eligible_df):.2%}\")\n",
    "    \n",
    "    return df.drop(columns=['Tc C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cddd8e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total eligible molecules: 291\n",
      "Global test set size: 29\n",
      "Global test fraction: 9.97%\n"
     ]
    }
   ],
   "source": [
    "df = generate_global_test(df, test_fraction=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "81c37188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('0_raw_data_sets/2_global_test_flagged_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96601438",
   "metadata": {},
   "source": [
    "### Feature selection and data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "46cd2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filename = \"0_raw_data_sets/2_global_test_flagged_data.csv\"\n",
    "df_orig = pd.read_csv(filename, index_col=0)\n",
    "\n",
    "\n",
    "output_dir = '2_split_datasets'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_split_dict = {\n",
    "    \"enthalpy\": df_orig.dropna(subset=['dH (kcal/mol)']).drop(columns=[col for col in df_orig.columns if \"dS (\" in col]),\n",
    "    \"entropy\": df_orig.dropna(subset=['dS (cal/mol/K)']).drop(columns=[col for col in df_orig.columns if \"dH (\" in col]),\n",
    "    \"has_both\": df_orig.dropna(subset=['dH (kcal/mol)', 'dS (cal/mol/K)']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "817d1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MI score and reduce and save datasets\n",
    "all_top_features = []\n",
    "\n",
    "for prop, df in df_split_dict.items():\n",
    "    monomer_states_for_splitting = df['BASE_Monomer_State']\n",
    "    polymn_cat = df['BASE_Category']\n",
    "    smiles_for_splitting = df['Canonical SMILES']\n",
    "    one_hot_encoding = pd.get_dummies(df[['BASE_Category', 'BASE_Monomer_State']])\n",
    "    global_test_set_flag = df['Global_Test']\n",
    "\n",
    "    if \"has_both\" not in prop:\n",
    "        # Mutual Information-based feature reduction\n",
    "        target = df.iloc[:, 1]\n",
    "        features = df.iloc[:, 19:-1]\n",
    "\n",
    "        mutual_info = mutual_info_regression(features, target, random_state=42)\n",
    "        mi_df = pd.DataFrame({'Feature': features.columns, 'MI_Score': mutual_info}).sort_values(by='MI_Score', ascending=False)\n",
    "\n",
    "        top_k_features = 80\n",
    "        top_features = mi_df.head(top_k_features)['Feature'].tolist()\n",
    "        all_top_features += top_features\n",
    "\n",
    "        reduced_features = features[top_features]\n",
    "        reduced_df = pd.concat([target, monomer_states_for_splitting, polymn_cat, smiles_for_splitting, global_test_set_flag, one_hot_encoding, reduced_features], axis=1)\n",
    "        reduced_df.to_csv(os.path.join(output_dir, f\"{prop}_{top_k_features}_MI_reduced.csv\"), index=True)\n",
    "    else:\n",
    "        targets = df.iloc[:, 1:3]\n",
    "        features = df.iloc[:, 20:-1]\n",
    "\n",
    "        targets['Tc (C)'] = targets['dH (kcal/mol)']*1000/targets['dS (cal/mol/K)'] - 273.15 # C\n",
    "        targets = targets.round(3)\n",
    "    \n",
    "        merged_top_features = list(set(all_top_features))\n",
    "        reduced_features = features[merged_top_features]\n",
    "        reduced_df = pd.concat([targets['Tc (C)'], monomer_states_for_splitting, polymn_cat, smiles_for_splitting, global_test_set_flag, one_hot_encoding, reduced_features], axis=1)\n",
    "        reduced_df.to_csv(os.path.join(output_dir, f\"{prop}_{len(merged_top_features)}_MI_reduced.csv\"), index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
